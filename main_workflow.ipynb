{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The AbspectroscoPY workflow: files required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file requires the following files:\n",
    "\n",
    "I.   TOOLBOX (\"abspectroscopy_functions.py\")\n",
    "\n",
    "II.  USER CONFIGURATION FILE (\"config.py\") \n",
    "     The file provides an example of configuration settings for the variables used in the functions that the user can modify.\n",
    "     \n",
    "III. CSV FILES from the folders \"data_scan_fp\" and \"other_data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The AbspectroscoPY workflow: contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. IMPORT RAW DATA FILES\n",
    "\n",
    "II. PREPROCESS THE DATASET\n",
    "\n",
    "    A) DATA TYPE CONVERSION\n",
    "    B) DATA QUALITY ASSESSMENT\n",
    "    C) TIME AXIS SHIFTING\n",
    "    D) ATTENUATION DATA CORRECTION\n",
    "    E) DATA SMOOTHING\n",
    "    \n",
    "III. EXPLORE THE DATASET\n",
    "    \n",
    "    A) VISUALISATION OF DATA DISTRIBUTION\n",
    "    B) OUTLIER/EVENT IDENTIFICATION AND REMOVAL\n",
    "    \n",
    "IV. INTERPRET THE RESULTS\n",
    "\n",
    "    A) ABSORBANCE RATIOS\n",
    "    B) ABSORBANCE SPECTRA CHANGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages, user configuration and AbspectroscoPY toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import sys\n",
    "import os   \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator)\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import datetime as dt\n",
    "import seaborn as sns\n",
    "from scipy import interpolate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy as sp \n",
    "import statistics \n",
    "from statistics import median\n",
    "from scipy.optimize import curve_fit\n",
    "from pylab import *\n",
    "import glob\n",
    "\n",
    "from config import *                     # Personal settings of local user\n",
    "import abspectroscopy_functions as abspy # Functions from the AbspectroscoPY toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the file location, generate the result folders and specify the following variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILE LOCATION\n",
    "pathwithlibraries = 'C:/Users/cace0002/AbspectroscoPY/'\n",
    "indata = os.path.join(pathwithlibraries,'data_scan_fp/')        # path where to search for the data\n",
    "indata_events = os.path.join(pathwithlibraries,'other_data/events_table.csv') # input event table\n",
    "\n",
    "### GENERAL VARIABLES\n",
    "col_sel = '255 nm'                                              # select a specific wavelength to plot\n",
    "timestart ='2018-11-12 16:04:00'                                # starting date\n",
    "timeend ='2018-12-03 20:44:00'                                  # ending date\n",
    "\n",
    "### TIME-AXIS SHIFTING\n",
    "nsamples_per_hour = 30                                          # sampling frequency (number of samples per hour)\n",
    "tshift = '0 hours 28 min'                                       # time shift not due to when the Daylight Saving Time ends\n",
    "tshift2 = '12 hours 00 min'                                     # time shift to compare measurements from different sensors\n",
    "\n",
    "### ATTENUATION DATA CORRECTION\n",
    "path_length = 3.5                                               # path length of the window of the sensor [cm]\n",
    "\n",
    "### OUTLIER/EVENT IDENTIFICATION AND REMOVAL\n",
    "splitstrs = ['2018-11-19 00:00:00', '2018-11-24 12:00:00']      # specify the dates you want to use to split the dataset in periods for the IQR method\n",
    "\n",
    "### INTERPRET THE RESULTS\n",
    "# abs_ratio:\n",
    "date_ref_start = '2018-11-13 05:02:00'                          # define reference period for computing the changes (%) ratio of absorbance data and spectral curve data\n",
    "date_ref_end = '2018-11-14 05:02:00'\n",
    "date_interesting = '2018-12-04 08:38:00'                        # define a date to look at the change in percentage at a certain date compared to the reference period\n",
    "\n",
    "# abs_spectral_curve\n",
    "col_sel_sc = '254.5'                                            # column to plot\n",
    "start_date = '2018-11-13 05:02:00'                              # define the range of dates for which we want to generate the spectral curve\n",
    "end_date = '2018-12-04 08:44:00'\n",
    "year = 'november_december_2018'                                 # title when saving the csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "root.attributes(\"-topmost\", True)\n",
    "\n",
    "MsgBox = tk.messagebox.askquestion ('Check','Have you specified the variables and the file location in cell 2?',icon = 'warning')\n",
    "if MsgBox == 'yes':  \n",
    "    root.destroy()\n",
    "else:\n",
    "    tk.messagebox.showwarning('Warning','Please specify them and restart the kernel!')  \n",
    "    root.destroy()\n",
    "    raise SystemExit('Stop right here and restart the kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "root.attributes(\"-topmost\", True)\n",
    "\n",
    "MsgBox = tk.messagebox.askquestion ('Check','Do you know that, besides these variables, you might have to modify the configuration file to adapt the scripts to your data (config.py)?',icon = 'question')\n",
    "if MsgBox == 'yes':  \n",
    "    root.destroy()\n",
    "else:\n",
    "    tk.messagebox.showinfo ( \"Info\", \"Please check and edit the file config.py\")   \n",
    "    root.destroy()\n",
    "    \n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "root.attributes(\"-topmost\", True)\n",
    "\n",
    "MsgBox = tk.messagebox.askquestion ('Check','Do you know that there is a file where to modify the functions, if it is not enough changing the configuration file (abspectroscopy_functions.py)?',icon = 'question')\n",
    "root.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here onwards no user input should be required, except for tshift_dst and abs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = os.path.join(pathwithlibraries,'results/')                         # create a new folder where to store the results\n",
    "if not os.path.exists(output):\n",
    "    os.mkdir(output)\n",
    "    \n",
    "output_outliers = os.path.join(output,'sr_periods_outliers/')               # create a new folder that will include the outlier files\n",
    "output_no_outliers = os.path.join(output,'sr_periods_no_outliers/')         # create a new folder that will include the files without outliers\n",
    "if not os.path.exists(output_outliers): \n",
    "    os.mkdir(output_outliers)\n",
    "if not os.path.exists(output_no_outliers):\n",
    "    os.mkdir(output_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. IMPORT RAW DATA FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET ASSEMBLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abs_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listoffileswithpath,listoffilesnopath = abspy.get_files_list(indata) # get the list of files with a specific pattern\n",
    "print('List of files including path:', '\\n', listoffileswithpath, '\\n')\n",
    "print('List of files without path:', '\\n', listoffilesnopath, '\\n')\n",
    "\n",
    "dateheadername = abspy.guess_date_column(listoffileswithpath, possibledateheadernames, header_rownr+1) # determine the name of the date column using a list of possible date column names \n",
    "print('Date header name:', dateheadername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init = abspy.abs_read(listoffileswithpath, listoffilesnopath, header_rownr, dateheadername, drop_col) # import the list of files\n",
    "df_init.to_csv(output + 'df_' + str(sample_name) + '.csv', sep = sep, decimal = decimal, index=True) # export the dataset\n",
    "\n",
    "df_init.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. PREPROCESS THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) DATA TYPE CONVERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert2dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtypecor = abspy.convert2dtype(df_init, dateheadername)\n",
    "df_dtypecor.to_csv(output + 'df_dtypecor_' + str(sample_name) + '.csv', sep = sep, decimal = decimal, index = True) # to export the type-converted dataset\n",
    "df_dtypecor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) DATA QUALITY ASSESSMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonan = df_dtypecor.copy()\n",
    "df_nonan = df_nonan.drop(df_nonan[df_nonan.isnull().all(axis=1)].index)  # drop rows containing only missing data\n",
    "df_nonan = df_nonan.dropna(axis=1, how='all', inplace=False) # drop columns containing only missing data\n",
    "df_nonan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nan_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_colper, nan_rowper = abspy.nan_check(df_nonan, dateheadername)\n",
    "nan_colper.to_csv(output + 'missing_data_per_column_percent.csv', sep = sep, decimal = decimal, index=True) # export the missing data per column and row as percentage\n",
    "nan_rowper.to_csv(output + 'missing_data_per_row_percent.csv', sep = sep, decimal = decimal, index=True)\n",
    "df_nonan.to_csv(output + 'df_nonan_' + str(sample_name) + '.csv', sep = sep, decimal = decimal, index=True) # export the dataframe without missing data\n",
    "nan_colper, nan_rowper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dup_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup, df_dup_all = abspy.dup_check(df_nonan, dateheadername)\n",
    "df_dup.to_csv(output + 'duplicates_by_date.csv', sep = sep, decimal = decimal, index=True) # export the duplicates by dateheadername \n",
    "df_dup_all.to_csv(output + 'duplicates.csv', sep = sep, decimal = decimal, index=True) # export the duplicates by all columns    \n",
    "df_dup, df_dup_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "plt.ion()\n",
    "\n",
    "# Run twice if the plot looks too small\n",
    "\n",
    "title1 =  'duplicates_'\n",
    "abspy.makeaplot(df_dup_all, output, col_sel, timestart, timeend, sample_name, title1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **drop_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodupall = df_nonan.drop_duplicates(subset=None, keep=\"first\", inplace=False) # drop second duplicate by all columns\n",
    "df_nodupall.to_csv(output + 'df_nodup_' + str(sample_name) + '.csv', sep = sep, decimal = decimal, index=True) # export the dataframe\n",
    "df_nodupall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) TIME AXIS SHIFTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tshift_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dst = df_nodupall.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "root.attributes(\"-topmost\", True)\n",
    "\n",
    "MsgBox = tk.messagebox.showwarning ('Warning','Do the data need to be time-shifted by Daylight Saving Time? Input yes or no in the following cell.', icon = 'warning')\n",
    "root.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = input()\n",
    "if answer == 'yes':\n",
    "    df_dst = abspy.tshift_dst(df_nodupall, dateheadername, nsamples_per_hour)\n",
    "    df_dst.to_csv(output + 'df_dst.csv', sep = sep, decimal = decimal, index=True) # export the dst time shifted dataset  \n",
    "    df_shifted = df_dst.copy()    \n",
    "elif answer == 'no':\n",
    "    df_shifted = df_dst.copy()\n",
    "else:\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes(\"-topmost\", True)\n",
    "\n",
    "    MsgBox = tk.messagebox.showerror ('Error','You are not providing one of the two possible answers. Please input \"yes\" or \"no\".',icon = 'error')\n",
    "    root.destroy()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip the next two cells if there is no extra time difference:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) shift the dataset in time, if there is any time difference between the sensor and the clock not due to when the Daylight Saving Time ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeshift = pd.Timedelta(tshift)\n",
    "df_shifted.index = df_shifted.index + timeshift\n",
    "df_shifted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) to be able to compare the sensors data of the surface water to the ones inside the plant shift the time one hour forward and account for the time the surface water needs to reach the treatment step (e.g. 11 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeshift2 = pd.Timedelta(tshift2)\n",
    "df_shifted.index = df_shifted.index + timeshift2\n",
    "df_shifted.to_csv(output + 'df_shifted_' + str(sample_name) + '.csv', sep = sep, decimal = decimal, index=True) # export the time shifted dataset \n",
    "df_shifted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) ATTENUATION DATA CORRECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abs_pathcor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abspy.abs_pathcor(df_shifted, path_length) [0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "root.attributes(\"-topmost\", True)\n",
    "\n",
    "MsgBox = tk.messagebox.showwarning ('Warning','Compare the attenuation value read by the sensor before and after path length correction for a specific date to the value obtained in the laboratory. Input yes or no in the following cell.',icon = 'warning')\n",
    "root.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = input()\n",
    "if answer == 'yes':\n",
    "    df_pathcor = abspy.abs_pathcor(df_shifted, path_length)[5] # the dataframe with corrected values is the fifth output of the function\n",
    "elif answer == 'no':\n",
    "    df_pathcor = df_shifted.copy()\n",
    "else:\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes(\"-topmost\", True)\n",
    "\n",
    "    MsgBox = tk.messagebox.showerror ('Error','You are not providing one of the two possible answers. Please input \"yes\" or \"no\".',icon = 'error')\n",
    "    root.destroy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pathcor.to_csv(output + 'df_pathcor_' + str(sample_name) + '.csv', sep = sep, decimal = decimal, index=True) # export the pathlength corrected dataset \n",
    "df_pathcor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abs_basecor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the absorbance spectra for different dates covering the temporal variability of the data and choose a wavelength range to correct for the baseline drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "abspy.makeabsplot(df_pathcor, output, dateparsingformat, nperiods, sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the baseline correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bc = abspy.abs_basecor(df_pathcor, startwv)\n",
    "df_bc.to_csv(output + 'df_baselinecor_' + str(sample_name) + '.csv', sep = sep, decimal = decimal, index=True) # export the dataframe\n",
    "df_bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E) DATA SMOOTHING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frequency_min = 60/nsamples_per_hour                # sampling frequency (measurement interval in minutes)\n",
    "\n",
    "median_window1 = median_window1_min / sample_frequency_min # median window from minute to number of samples\n",
    "median_window2 = median_window2_min / sample_frequency_min\n",
    "median_window3 = median_window3_min / sample_frequency_min\n",
    "median_window_selected = median_window_min_selected / sample_frequency_min\n",
    "print('number of samples for window 1:', median_window1)\n",
    "print('number of samples for window 2:', median_window2)\n",
    "print('number of samples for window 3:', median_window3)\n",
    "print('number of samples for window selected:', median_window_selected)\n",
    "\n",
    "df_median1 = df_bc.copy()\n",
    "median_window1_int=int(median_window1)\n",
    "df_median1 = df_median1.rolling(median_window1_int, center=True, axis=0).median()# compute the rolling median of the absorbance series\n",
    "df_median1 = df_median1.dropna(axis=0)                                           # drop rows with NaN due to rolling median calculation\n",
    "\n",
    "df_median2 = df_bc.copy()\n",
    "median_window2_int=int(median_window2)\n",
    "df_median2 = df_median2.rolling(median_window2_int, center=True, axis=0).median()\n",
    "df_median2 = df_median2.dropna(axis=0) \n",
    "\n",
    "df_median3 = df_bc.copy()\n",
    "median_window3_int=int(median_window3)\n",
    "df_median3 = df_median3.rolling(median_window3_int, center=True, axis=0).median()\n",
    "df_median3 = df_median3.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "abspy.makerollplot(df_median1, df_median2, df_median3, output, col_sel, timestart, timeend, sample_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_median_selected = df_bc.copy()\n",
    "median_window_selected_int=int(median_window_selected)\n",
    "df_median_selected = df_median_selected.rolling(median_window_selected_int, center=True, axis=0).median()\n",
    "df_median_selected = df_median_selected.dropna(axis=0) \n",
    "df_median_selected.to_csv(output + 'df_mediancor_' + str(sample_name) + '_' + str(median_window_selected) + '.csv', sep = sep, decimal = decimal, index=True) # export the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. EXPLORE THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) VISUALISATION OF DATA DISTRIBUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **kdeplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_median = df_median_selected.copy()\n",
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "abspy.makeakdeplot(df_median_selected, output, sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) OUTLIER/EVENT IDENTIFICATION AND REMOVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abs_slope_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr = abspy.abs_slope_ratio(df_median, S_f)\n",
    "df_sr[1:].to_csv(output + 'df_sr_' + str(sample_name) + '_' + str(S_f) +'.csv', index = True, sep = ';') # export the  dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "title1 = 'slope_ratio_'\n",
    "abspy.makeaplot(df_sr, output, 'SR', timestart, timeend, sample_name, title1, ylabel = 'Slope ratio [dimensionless]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outlier_id_drop_iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr2 = df_sr.copy()\n",
    "dflist, out1, out2 = abspy.outlier_id_drop_iqr(df_sr2, output_no_outliers, output_outliers, splitstrs, timestart, timeend, dateparsingformat, sample_name)\n",
    "print('Lower and upper limits of the interquartile range:', '\\n', out1, '\\n', 'outlier percentage:', out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abspy.makeaoutplot(dflist, output_no_outliers, output, sr_col, timestart, timeend, dateparsingformat, splitstrs, sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outlier_id_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ev, df_drop = abspy.outliers_id_drop(df_bc, indata_events, output, col_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "title1 = 'absorbance_data_baseline_corrected_with_events'  \n",
    "abspy.makeaplotev(df_bc, df_ev, output, col_sel, timestart, timeend, dateparsingformat, sample_name, title1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "title2 = 'absorbance_data_baseline_corrected_with_no_events' \n",
    "abspy.makeaplotev(df_drop, df_ev, output, col_sel, timestart, timeend, dateparsingformat, sample_name, title2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. INTERPRET THE RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abs_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratio, df_change_per = abspy.abs_ratio(df_median, date_ref_start, date_ref_end, dateparsingformat, wv1, wv2, date_interesting)\n",
    "df_change_per.to_csv(output + 'df_absorbance_ratio_change_' + str(wv1) + '_' + str(wv2) + '.csv', index = True) # export the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "title1 = 'absorbance_ratio' + str(wv1) + '_' + str(wv2)\n",
    "abspy.makeaplot_nocolsel(df_change_per, output, timestart, timeend, wv1, wv2, sample_name, title1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "title2 = 'absorbance_ratio_change_' + str(wv1) + '_' + str(wv2)\n",
    "abspy.makeaplot_nocolsel(df_change_per, output, timestart, timeend, wv1, wv2, sample_name, title2, ylabel = '{} {} '.format('A' + '$_{' + wv1 + '}$' + '/' + 'A' + '$_{' + wv2 + '}$', 'change [%]')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abs_fit_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_par_no_zero, exp_par_std_no_zero = abspy.abs_fit_exponential(df_median, startwl, endwl, wl0, S_f)\n",
    "exp_par_no_zero[1:].to_csv(output +'exp_fit_' + str(sample_name) + '_' + str(wl0) + '_' + str(S_f) +'.csv', index = True, sep = ';')\n",
    "exp_par_std_no_zero[1:].to_csv(output + 'exp_fit_std_' + str(sample_name) + '_' + str(wl0) + '_' + str(S_f) +'.csv', index = True, sep =';')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "abspy.abs_fit_exponential_plot(df_median, output, dateparsingformat, date_interesting, startwl, endwl, wl0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abs_spectral_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_data_r2, sc_data_r2_no_NaN = abspy.abs_spectral_curve(df_median, start_date, end_date, dateparsingformat, S_f, r2threshold)\n",
    "sc_data_r2.to_csv(output + 'df_sc_' + str(sample_name) + '_' + str(S_f) + '_' + str(year) +'.csv', index = True, sep = ';')\n",
    "sc_data_r2_no_NaN.to_csv(output + 'df_sc_no_NaN_' + str(sample_name) + '_' + str(S_f) + '_' + str(year) +'.csv', index = True, sep = ';') # to drop the columns with any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "title1 = 'spectral_curve_'\n",
    "abspy.makeaplot(sc_data_r2_no_NaN, output, col_sel_sc, timestart, timeend, sample_name, title1, ylabel = 'Negative spectral slope [nm $^{-1}$]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral slope changes [%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc_change_per = abspy.abs_spectral_curve_perchanges(sc_data_r2_no_NaN, date_ref_start, date_ref_end, dateparsingformat)\n",
    "df_sc_change_per.to_csv(output + 'df_sc_change_' + str(sample_name) + '_' + str(S_f) + '_' + str(year) +'.csv', index = True) # export the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "title2 = 'spectral_curve_change_'\n",
    "abspy.makeaplot(df_sc_change_per, output, col_sel_sc, timestart, timeend, sample_name, title2, ylabel = 'Negative spectral slope change [%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "root.attributes(\"-topmost\", True)\n",
    "\n",
    "MsgBox = tk.messagebox.showinfo ('Info','If the plots are not visible or if they are too small run again the individual cell.',icon = 'info')\n",
    "root.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "root.attributes(\"-topmost\", True)\n",
    "\n",
    "MsgBox = tk.messagebox.showinfo ('Info', 'Thank you for using AbspectroscoPY. If you find any bug or you would like to improve the scripts, please propose changes on GitHub or write to claudia.cascone@gmail.com.', icon = 'info')\n",
    "root.destroy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
